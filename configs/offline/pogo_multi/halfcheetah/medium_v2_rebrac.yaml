# POGO Multi-Actor with ReBRAC algorithm (JAX)
# ReBRAC의 구조를 그대로 사용하되, Actor만 multi-actor

dataset_name: halfcheetah-medium-v2
train_seed: 0
eval_seed: 42

# POGO Multi-Actor 설정
# w2_weights: Actor1부터의 가중치 리스트 (Actor0는 W2 penalty 없음)
w2_weights: [10.0, 10.0]
num_actors: 3
# actor_configs: 각 actor의 타입 지정
#   - "gaussian": Gaussian policy (mean에 tanh 적용된 상태에서 샘플링, closed form W2 사용)
#   - "tanh_gaussian": TanhGaussian policy (unbounded Gaussian에서 샘플링 후 tanh 적용, Sinkhorn 사용)
#   - "stochastic": Stochastic policy (Sinkhorn distance 사용)
#   - "deterministic": Deterministic policy (L2 distance 사용)
actor_configs:
  - type: deterministic  # Actor0: deterministic
  - type: gaussian       # Actor1: Gaussian (closed form W2)
  - type: gaussian       # Actor2: Gaussian (closed form W2)

# Sinkhorn 설정 (Actor1+용)
sinkhorn_K: 4
sinkhorn_blur: 0.05
sinkhorn_backend: "auto"

# ReBRAC 파라미터 (ReBRAC 구조 그대로 사용)
actor_learning_rate: 1e-3
critic_learning_rate: 1e-3
hidden_dim: 256
actor_n_hiddens: 3
critic_n_hiddens: 3
gamma: 0.99
tau: 5e-3
actor_bc_coef: 1.0
critic_bc_coef: 1.0
actor_ln: false
critic_ln: true
policy_noise: 0.2
noise_clip: 0.5
policy_freq: 2
normalize_q: true

# Training params
batch_size: 1024
num_epochs: 1000
num_updates_on_epoch: 1000
normalize_reward: false
normalize_states: false

# Evaluation params
eval_episodes: 10
eval_every: 5

# Wandb
project: PORL
group: pogo-multi-rebrac
name: pogo-multi-rebrac
